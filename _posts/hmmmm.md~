It is a rather common real-life situation one where some piece of information cannot be directly accessed but rather it is inferred by looking at some other information we have, more accessible or readily available. And more often than not, this inference has to be repeated step after step to drive some computation. Very popular examples here are speech recognition and robot movement control:

* I don't know *exactly* what word you are spelling out, but by looking at the waveform it *sounds* like...

* our friend the robot does not know where he is, though its sensors tell it is quite close to a wall

 And very common applications always have very cool, and established, models. The model of reference here is that of [hidden Markov models](https://en.wikipedia.org/wiki/Hidden_Markov_model) and you can find of course pages galore on the topic. In a nutshell:

use sandbox https://en.wikipedia.org/w/index.php?title=Wikipedia:Sandbox&action=submit
inline with ![alt text](http://monosnap.com/image/bOcxxxxLGF.png "Title")

---latex---: X_t, we define our hidden state at time t as a random variable; the process unfolds as an infinite sequence X_0, X_1, ... of such variables 
 
---latex---: Markov rule: P(X_t|X_0,...,X_t-1)=P(X_t|X_t-1). This is the so called *memorylessness*, we just look at the immediately preceding state to pontificate on the one to come.

---latex---: Evolution: at every step t for every pair of states (i,j), in a possibly continuous space, we pass from state i to state j with probability PP(X_t=j|X_t-1=i).

---latex---: At every step we read a value from the indicator variable, whose response is supposed to be meaningfully linked to the hidden state, so something like P(X_t=i|E_k=e) for every state-evidence pair (i, e).

In the worst case scenarios (read as: always in real-life) the evolution probabilities may not be easily estimated. In:

---latex---: X_t=f_t(X_t-1,W_t-1)

where W models noise on the states, may be nonlinear and time-dependent. The same in general can be said of a function h_k relating the evidence to the hidden state.

We define the *filtering problem* as the estimation of the current state x_t given the previous state x_t-1 and all of the previous pieces of evidence e_1,...,e_t-1. In a regular [bayesian](https://en.wikipedia.org/wiki/Bayes_theorem) the estimation of the state is actually a probability distribution:
P(X_t|E_1,...,E_t-1)
computed in two steps:

*prediction, calculate P(X_t|E_1,...,E_t-1), the prior over X_t before receiving the evidence score e_t in the...

*update, the posterior on X_t is obtained by direct application of the Bayes rule

These two steps all boil down to integrals, which we don't expect to be amenable analytically. So what do we do? If we can't snipe the target then let's just shoot around at random - that's the spirit of the so-called [Monte Carlo](https://en.wikipedia.org/wiki/Monte_Carlo_method) methods! An option is the *bootstrap particle filtering*. The *particle* in it refers to the fact that we create and maintain a population of random (MC style) particles along the simulation, and [*bootstrap*](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)) because it makes use of sampling with replacement. The point here is that the particles, at every step, represent the state posterior distribution, in the form of a sample rather than a closed form. 

It turns out that the algorithm is pretty simple:

1. Extract n samples from the initial prior X_0, these are our particles P_0={x_00, x_01, ... x_0n}; these particles are realizations of the variable X_0

2. For each particle in P_i sample P(X_t|X_t-1), so basically choose how to evolve each sample according to the f_t above

3. How likely is each evolution? Lookup every score with the evidence function h_j, which is sampling the distribution P(X_t|E_t); these values may be one-normalized and represent a weighthing on the particles population

4. Bootstrap step, resample with replacement according to the weights; this prepares the population of particles for the next iteration

In the case of discrete distributions, all of the information may be conveniently encoded in matrices and vectors:

* a matrix M_ij where the value is the probability of transitioning from state i into state j

* a matrix E_kl where the value is the probability of the hidden state k given the observable state l

